{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fc00a60-d0e2-4f58-9eee-6d9b1eee8f3a",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ffe468-02a4-4560-967d-fa1aedf6f22a",
   "metadata": {},
   "source": [
    "#### Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. \n",
    "#### The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f65b89e-f7b3-45cc-a4a5-e2c5b85fa95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready for ML!\n",
      "PyTorch version: 2.8.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "import torch\n",
    "\n",
    "print(\"Environment ready for ML!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e3899a-e447-40ba-9d7f-462512309be1",
   "metadata": {},
   "source": [
    "#### Type: Supervised Learning\n",
    "\n",
    "#### Problem: Both Classification and Regression\n",
    "\n",
    "#### Evaluation Metrics: Accuracy (classification), MSE (regression), feature importance\n",
    "\n",
    "#### Real-World Examples: Medical diagnosis, credit risk assessment, customer segmentation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f6f8fa-29ac-4b18-9f71-5a2f47cd2aca",
   "metadata": {},
   "source": [
    "#### Advantages\n",
    "\n",
    "- Interpretable and visualizable.\n",
    "\n",
    "- Requires minimal preprocessing (no normalization or dummy encoding).\n",
    "\n",
    "- Handles numeric inputs; categorical support is limited.\n",
    "\n",
    "- Fast prediction (logarithmic time per sample).\n",
    "\n",
    "- Supports multi-output tasks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed4d224-3e60-4ae0-bbf2-60ee96ea2ab6",
   "metadata": {},
   "source": [
    "Overview of `sklearn.tree` (scikit-learn decision-tree module):\n",
    "\n",
    "**Purpose and Scope**\n",
    "Supports supervised learning via decision trees for classification and regression. Creates simple if-then-else rules to predict outcomes based on feature splits. Non-parametric.([Scikit-learn][1])\n",
    "\n",
    "---\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "* Interpretable and visualizable.\n",
    "* Requires minimal preprocessing (no normalization or dummy encoding).\n",
    "* Handles numeric inputs; categorical support is limited.\n",
    "* Fast prediction (logarithmic time per sample).\n",
    "* Supports multi-output tasks.([Scikit-learn][1])\n",
    "\n",
    "---\n",
    "\n",
    "**Drawbacks**\n",
    "\n",
    "* Prone to overfitting if unrestricted.\n",
    "* Requires pruning or limiting depth, leaf count, or sample requirements to generalize.([Scikit-learn][1])\n",
    "\n",
    "---\n",
    "\n",
    "**Key Classes & Functions**\n",
    "\n",
    "* `DecisionTreeClassifier` – for classification tasks.\n",
    "* `DecisionTreeRegressor` – for regression tasks.([Scikit-learn][2])\n",
    "\n",
    "Important parameters for both:\n",
    "\n",
    "* `criterion`: `\"gini\"`, `\"entropy\"`, or `\"log_loss\"` – measure split quality.([Scikit-learn][3])\n",
    "* `splitter`: `\"best\"` or `\"random\"`.\n",
    "* `max_depth`, `min_samples_split`, `min_samples_leaf`: control tree shape.\n",
    "* `max_features`, `max_leaf_nodes`, `min_impurity_decrease`: add control over splits.([Scikit-learn][3])\n",
    "\n",
    "---\n",
    "\n",
    "**Visualization & Export**\n",
    "\n",
    "* `plot_tree(...)`: renders tree graphically with options for depth control, node labels, and aesthetics.([Scikit-learn][4])\n",
    "* `export_graphviz`, `export_text`: textual or Graphviz formats.([Scikit-learn][2])\n",
    "\n",
    "---\n",
    "\n",
    "**Introspection Tools**\n",
    "\n",
    "* `tree_.value`: class distribution per node. Relative values need multiplication by `n_node_samples` for absolute counts.([Stack Overflow][5])\n",
    "* `decision_path(X)`, `apply(X)`: trace samples through the tree or get leaf indices.([Scikit-learn][6])\n",
    "\n",
    "---\n",
    "\n",
    "**Regression Example Insight**\n",
    "Low `max_depth` yields smoother, generalized predictions. High depth overfits noise.([Scikit-learn][7])\n",
    "\n",
    "---\n",
    "\n",
    "**Related Ensemble Methods**\n",
    "Module includes wrapper for ensembles (Random Forests, Extra-Trees) to reduce overfitting and improve stability.([Scikit-learn][8])\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "Decision trees offer clear rules, flexibility, and easy visualization. They require tuning to avoid overfitting. scikit-learn supports these with configurable parameters, visualization tools, node-level introspection, and ensemble options.\n",
    "\n",
    "Let me know if you want concise code templates or deeper dive on tuning metrics.\n",
    "\n",
    "[1]: https://scikit-learn.org/stable/modules/tree.html?utm_source=chatgpt.com \"1.10. Decision Trees - Scikit-learn\"\n",
    "[2]: https://scikit-learn.org/stable/api/sklearn.tree.html?utm_source=chatgpt.com \"sklearn.tree — scikit-learn 1.7.1 documentation\"\n",
    "[3]: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html?utm_source=chatgpt.com \"DecisionTreeClassifier — scikit-learn 1.7.1 documentation\"\n",
    "[4]: https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html?utm_source=chatgpt.com \"plot_tree — scikit-learn 1.7.1 documentation\"\n",
    "[5]: https://stackoverflow.com/questions/47719001/what-does-scikit-learn-decisiontreeclassifier-tree-value-do?utm_source=chatgpt.com \"What does scikit-learn DecisionTreeClassifier.tree_.value do?\"\n",
    "[6]: https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html?utm_source=chatgpt.com \"Understanding the decision tree structure - Scikit-learn\"\n",
    "[7]: https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html?utm_source=chatgpt.com \"Decision Tree Regression — scikit-learn 1.7.1 documentation\"\n",
    "[8]: https://scikit-learn.org/stable/modules/ensemble.html?utm_source=chatgpt.com \"1.11. Ensembles: Gradient boosting, random forests, bagging, voting ...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe36c0b-8069-4200-ae32-bb4070242ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
